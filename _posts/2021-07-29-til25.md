---
published: true
layout: single
title:  "[TIL] 21/06/29 임베딩"
header:
  overlay_image: /images/unsplash-image-2.jpg
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
  actions:
    - label: "Learn more"
      url: ""
      
categories: [TIL]
tags: []
comments: true
---

오늘은 벡터 정보에 의미를 부여하는 방법인 임베딩에 대해서 공부했습니다. 

&nbsp;

&nbsp;

# 벡터에 의미를 부여하는 방법: 임베딩

대표적인 임베딩 방법들을 정리하면 다음과 같습니다. 

|구분|bag of words 가정|언어 모델|분포 가정|
|---|---|---|---|
|내용|어떤 단어들이 (많이) 쓰였는가|단어가 어떤 순서로 쓰였는가|어떤 단어가 같이 쓰였는가|
|대표 통계량|TF-IDF|-|PMI|
|대표모델|Deep Averageing Network|ELMo, GPT| Word2Vec|

## bag of words

저자가 생각한 주제가 문서에서의 단어 사용에 녹아있다 가정합니다.  
- 주제가 비슷한 문서들은 **단어의 빈도나 등장여부**가 중요하게 작용할 것입니다.  
- **TF-IDF**: 특정 문서에만 있으면서 많은 빈도 수를 보이는 단어를 높은 점수로 평가합니다. 

## 언어 모델
단어 시퀀스에 확률을 부여합니다. 
- 시퀀스 정보를 학습합니다. **다음 단어의 확률을 추정**해서 어떤 단어가 자연스러운지 추정합니다. 
- 대표적인 방법으로 **n-gram**이 있습니다. 
- 한번도 등장하지 않은 단어의 확률이 0으로 되는 문제점이 존재합니다. 이와 같은 경우는 **백오프**나 **스무딩** 기법 등을 사용해 해결합니다. 
- 특정 단어를 가리고 이를 추정하는 것을 학습시키는 **마스크 언어 모델**이 많이 사용되고 있습니다.(BERT 등)

## 분포 가정
특정 크기의 윈도우 내에 동시에 등장하는 이웃 단어 또는 문맥의 집합을 보고 단어가 문장 내에서 어디에 위치하는지 어떤 단어 쌍이 등장하는지 확인합니다. 
- **점별 상호 정보량 (PMI)** 과 연관이 있습니다. 
- 점별 상호 정보량은 단어의 등장여부가 독립인 경우 0, 서로 다른 단어가 항상 같이 등장한다면 1의 값을 지닙니다. 
- **BOW, skip-gram, Word2Vec** 등의 모델이 분포 가정을 사용합니다. 

# 말뭉치 

좋은 임베딩을 하기 위해서는 데이터를 확보해야 합니다.  
 